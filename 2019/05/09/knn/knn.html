<hr>
<p>title: K近邻算法的实现<br>tpye: post<br>data: 2019/4/29<br>categories: 机器学习</p>
<h2 id="mathjax-true"><a href="#mathjax-true" class="headerlink" title="mathjax: true"></a>mathjax: true</h2><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>k 临近算法，是机器学习（监督学习）的一种经典的分类算（当然也能解决其他的一些问题，例如线性回归）</p>
<p>官方解释：存在一个样本数据集，也称作训练样本集，并且样本中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系，输入没有标签的新数据后，将新数据的每个特征与样本集中的数据对应的特征进行比较，然后算法提取样本集中特征最相似的数据（最近邻）的分类标签。一般来说，我们只选择样本集中前 k 个最相似的数据，这就是 k-近邻算法中 k 的出处，通常 k 是不大于 20 的整数，最后，选择 k 个最相似的数据中出现次数最多的分类，作为新数据的分类。</p>
<p>我的理解：将原有样本集视为一个模型（KNN 非常特殊，可以视为没有，为了和其他模型统一，我把原本的数据集理解为一个模型），对于来一个样本，把他放在原数据集中，从它临近的类别来判断它的类别，就好像一朵花周围都是鸢尾花，那这朵花大概率也是鸢尾花（当然鸢尾花也有各种类别，下文就将拿鸢尾花这个经典的数据集来测试我们实现的 K 近邻算法），这里的 K 就是考虑的近邻的个数，是一个超参数。而算法的目的就是找出目标样本最近的 K 个样本，分析各个样本的类别，来决定样本的类别。</p>
<!-- more -->
<h1 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h1><h4 id="测试数据来源与加载"><a href="#测试数据来源与加载" class="headerlink" title="测试数据来源与加载"></a>测试数据来源与加载</h4><p>这里，我们使用 sklearn 封装好的数据集。<br>sklearn 的数据集有好多个种</p>
<ul>
<li>自带的小数据集（packaged dataset）：sklearn.datasets.load_<name></name></li>
<li>可在线下载的数据集（Downloaded Dataset）：sklearn.datasets.fetch_<name></name></li>
<li>计算机生成的数据集（Generated Dataset）：sklearn.datasets.make_<name></name></li>
<li>svmlight/libsvm 格式的数据集:sklearn.datasets.load_svmlight_file(…)</li>
<li>从买了 data.org 在线下载获取的数据集:sklearn.datasets.fetch_mldata(…)</li>
</ul>
<p>其中自带的小数据集中有</p>
<p><img src="https://raw.githubusercontent.com/sylarchen1389/sylarchen1389.github.io/master/image/578330-20170610195748356-1518693081.png" width="80%" height="80%" div="" align="center"></p>
<p>这些数据集都可以在官网上查到，以鸢尾花为例，可以在官网上找到 demo，<a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html">http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html</a></p>
<p>我们使用鸢尾花的数据集</p>
<pre><code class="py"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets
iris=datasets.load_iris()
</code></pre>
<p>使用<code>iris.key()</code>我们可以看到输出了<br><img src="iris_1.png" alt="iris_1"></p>
<p><code>DESCR</code>是特征的描述</p>
<p><code>feature_names</code>是特征的名字</p>
<p><code>data</code>是特征数据</p>
<p><code>target_names</code>是类别的名称</p>
<p><code>target</code>在此数据集中是鸢尾花的类别</p>
<p>方便后面操作，我们令</p>
<pre><code class="py">X = iris.data
y = iris.target
</code></pre>
<p>可以看到：</p>
<p><img src="iris_2.png" alt="iris_2"></p>
<p>这个数据集有 150 个样本，4 个特征，可以看到，X 与 Y 的数目是对应的</p>
<p>关于这个数据集的详细内容，可以通过<br><code>print(iris.DESCR)</code><br>来查看，详细内容在 sklearn 官网都有</p>
<pre><code class="py">OUTPUT：

    .. _iris_dataset:

    Iris plants dataset
    --------------------

    **Data Set Characteristics:**

        :Number of Instances: <span class="number">150</span> (<span class="number">50</span> <span class="keyword">in</span> each of three classes)
        :Number of Attributes: <span class="number">4</span> numeric, predictive attributes <span class="keyword">and</span> the <span class="class"><span class="keyword">class</span></span>
<span class="class">        :</span>Attribute Information:
            - sepal length <span class="keyword">in</span> cm
            - sepal width <span class="keyword">in</span> cm
            - petal length <span class="keyword">in</span> cm
            - petal width <span class="keyword">in</span> cm
            - <span class="class"><span class="keyword">class</span>:</span>
                    - Iris-Setosa
                    - Iris-Versicolour
                    - Iris-Virginica

        :Summary Statistics:

        ============== ==== ==== ======= ===== ====================
                        Min  Max   Mean    SD   Class Correlation
        ============== ==== ==== ======= ===== ====================
        sepal length:   <span class="number">4.3</span>  <span class="number">7.9</span>   <span class="number">5.84</span>   <span class="number">0.83</span>    <span class="number">0.7826</span>
        sepal width:    <span class="number">2.0</span>  <span class="number">4.4</span>   <span class="number">3.05</span>   <span class="number">0.43</span>   <span class="number">-0.4194</span>
        petal length:   <span class="number">1.0</span>  <span class="number">6.9</span>   <span class="number">3.76</span>   <span class="number">1.76</span>    <span class="number">0.9490</span>  (high!)
        petal width:    <span class="number">0.1</span>  <span class="number">2.5</span>   <span class="number">1.20</span>   <span class="number">0.76</span>    <span class="number">0.9565</span>  (high!)
        ============== ==== ==== ======= ===== ====================

        :Missing Attribute Values: <span class="keyword">None</span>
        :Class Distribution: <span class="number">33.3</span>% <span class="keyword">for</span> each of <span class="number">3</span> classes.
        .....
</code></pre>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h4 id="距离的定义"><a href="#距离的定义" class="headerlink" title="距离的定义"></a>距离的定义</h4><p>首先，K 近邻算法需要找到离预测样本最近的 K 个样本，那么我们就需要先明确什么是距离</p>
<p>对于大部分人来说，我们最熟悉就是欧拉距离了<br>$$\sqrt[2]{\sum_{i=0}^N(x_i^{(a)} - x_i^{(b)})^2}$$</p>
<p>将指数改为 1，我们就得到了曼哈顿距离<br>$$\sum_{i=0}^N\ |x_i^{(a)} - x_i^{(b)}|$$</p>
<p>读到这里，大家或许在想，为什么我们不干脆改成 n？没错，这时候我们得到了明可夫斯基距离，同时获得了一个超参数 n<br>$$\sqrt[n]{\sum_{i=0}^N(x_i^{(a)} - x_i^{(b)})^n}$$<br>下面的代码实现，我用大家都比较熟悉的欧拉距离实现</p>
<h4 id="Code-python"><a href="#Code-python" class="headerlink" title="Code (python)"></a>Code (python)</h4><pre><code class="py"><span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">from</span> math <span class="keyword">import</span> sqrt
<span class="keyword">from</span> collections <span class="keyword">import</span> Counter

<span class="class"><span class="keyword">class</span> <span class="title">KNNClassifier</span>:</span>

    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span>
        <span class="string">"""初始化kNN分类器"""</span>
        <span class="keyword">assert</span> k &gt;= <span class="number">1</span>, <span class="string">"k must be valid"</span>
        self.k = k
        <span class="string">"""需要注意的是，因为kNN是典型的非参数\</span>
<span class="string">        学习算法，对于这个算法，我们需要有成员\</span>
<span class="string">        来储存已有的数据"""</span>
        self._X_train = <span class="keyword">None</span>
        self._y_train = <span class="keyword">None</span>

    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X_train, y_train)</span>:</span>
        <span class="string">"""根据训练数据集X_train和y_train训练kNN分类器"""</span>
        <span class="keyword">assert</span> X_train.shape[<span class="number">0</span>] == y_train.shape[<span class="number">0</span>], \
            <span class="string">"the size of X_train must be equal to the size of y_train"</span>
        <span class="keyword">assert</span> self.k &lt;= X_train.shape[<span class="number">0</span>], \
            <span class="string">"the size of X_train must be at least k."</span>

        self._X_train = X_train
        self._y_train = y_train
        <span class="keyword">return</span> self

    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_predict)</span>:</span>
        <span class="string">"""给定待预测数据集X_predict，返回表示X_predict的结果向量"""</span>
        <span class="keyword">assert</span> self._X_train <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self._y_train <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, \
                <span class="string">"must fit before predict!"</span>
        <span class="keyword">assert</span> X_predict.shape[<span class="number">1</span>] == self._X_train.shape[<span class="number">1</span>], \
                <span class="string">"the feature number of X_predict must be equal to X_train"</span>

        y_predict = [self._predict(x) <span class="keyword">for</span> x <span class="keyword">in</span> X_predict]
        <span class="keyword">return</span> np.array(y_predict)

    <span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(self, x)</span>:</span>
        <span class="string">"""给定单个待预测数据x，返回x的预测结果值"""</span>
        <span class="keyword">assert</span> x.shape[<span class="number">0</span>] == self._X_train.shape[<span class="number">1</span>], \
            <span class="string">"the feature number of x must be equal to X_train"</span>

        distances = [sqrt(np.sum((x_train - x) ** <span class="number">2</span>))
                     <span class="keyword">for</span> x_train <span class="keyword">in</span> self._X_train]
        nearest = np.argsort(distances)

        topK_y = [self._y_train[i] <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:self.k]]
        votes = Counter(topK_y)

        <span class="keyword">return</span> votes.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]

<span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(y_true, y_predict)</span>:</span>
    <span class="string">'''计算y_true和y_predict之间的准确率'''</span>
    <span class="keyword">assert</span> y_true.shape[<span class="number">0</span>] == y_predict.shape[<span class="number">0</span>], \
    <span class="string">"the size of y_true must be equal to the size of y_predict"</span>

    <span class="keyword">return</span> sum(y_true == y_predict) / len(y_true)


    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, X_test, y_test)</span>:</span>
        <span class="string">"""根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""</span>

        y_predict = self.predict(X_test)
        <span class="keyword">return</span> accuracy_score(y_test, y_predict)

    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span>
        <span class="keyword">return</span> <span class="string">"KNN(k=%d)"</span> % self.k
</code></pre>
<h1 id="测试我们的算法"><a href="#测试我们的算法" class="headerlink" title="测试我们的算法"></a>测试我们的算法</h1><h3 id="数据分割（train-test-split）"><a href="#数据分割（train-test-split）" class="headerlink" title="数据分割（train_test_split）"></a>数据分割（train_test_split）</h3><p>这里做一个操作，将原先的数据集 X 和 y 按照 0.2 分割成 X_train, X_test, y_train, y_test<br>（len(x_train) = 0.8 *len(x)）,方便后面对我们的算法进行简单的测试</p>
<p><img src="train.png" alt="train"></p>
<p>代码就不贴在这了，有兴趣的读者可以访问我的 github 查看</p>
<h3 id="训练（拟合）"><a href="#训练（拟合）" class="headerlink" title="训练（拟合）"></a>训练（拟合）</h3><p>我们先声明一个我们封装的类的对象，<br>再调用 fit 函数训练</p>
<pre><code class="py">my_knn_clf = KNNClassifier(k=<span class="number">3</span>)
my_knn_clf.fit(X_train,y_train)
</code></pre>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>直接调用我们写的 predict 函数</p>
<pre><code class="py">y_predict=my_knn_clf.predict(X_test)
</code></pre>
<p><img src="predict.png" alt="predict"></p>
<h1 id="评测（正确率）"><a href="#评测（正确率）" class="headerlink" title="评测（正确率）"></a>评测（正确率）</h1><p>对于鸢尾花这个数据集，最简单的计算计算计算正确率的方法就是与原数据中的 y 比较，一样则分类正确。我们将其加起来，再除以分母，就得到了一个简单的评判标准。<br><img src="score.png" alt="score"></p>
<h1 id="调用-sklearn-实现-KNN"><a href="#调用-sklearn-实现-KNN" class="headerlink" title="调用 sklearn 实现 KNN"></a>调用 sklearn 实现 KNN</h1><p>sklearn 是一个机器学习的库，封装了许多机器学习算法，我们可以直接通过调库来使用相关的算法</p>
<p>我们通过<code>from sklearn.neighbors import KNeighborsClassifier</code>来加载 sklearn 封装的 KNN 分类器</p>
<p>然后与上面的操作相同，我们先声明对象，再调用它封装的 fit 函数训练</p>
<p><img src="sk_knn.png" alt="sk_knn"></p>
<p>同样，sklearn 也为分类器封装了 predict 函数，我们可以直接调用 predict 函数来对预测样本进行分类</p>
<p><img src="sk_knn_2.png" alt="sk_knn_2"></p>
<p>我们调 sklearn 封装的 score 函数来测试算法的正确率</p>
<p><img src="sk_knn_3.png" alt="sk_knn_3"></p>
<p>当然，sklearn 对 score 函数的实现方式不同，所以评价标准也不同，虽然我们自己实现的 kNN 比 sklearn 的 kNN 得分高，但我们不能说我们的算法比 sklearn 封装的算法好，因为评判标准是不一样的。实际上，如何选择出对于当前问题最合适的算法，也是机器学习的一个难题</p>
<h1 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h1><p>超参数的调节是机器学习的一个重要问题，下面举了几个我们实现的算法中出现的超参数，对于如何获取这些超参数的最优值，我会用一些比较简单但时间复杂度可能稍高的方法，其他方法请有兴趣的读者们自行查找</p>
<p>这里对于超参数的搜索，我们使用 sklearn 封装的 KNN 算法.</p>
<h4 id="k"><a href="#k" class="headerlink" title="k"></a>k</h4><p>最明显的超参数，就是我<br>们要搜索的近邻的个数 k，到底要检测周围多少个点，才能是我们的算法得到最好的正确率？</p>
<p>sklearn 中使用的变量名为 n_neighbor</p>
<p>我们可以简单地写一个 for 循环测试一下<br><img src="for.png" alt="for"></p>
<p>可以看到这里显示最好的 k 是 4，而准确率是 0.99166…..</p>
<h4 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h4><p>我们的算法寻找 k 个最近的近邻的标签进行比较，那么距离的远近是否需要考虑呢？比如距离大一点的样本，对预测的样本的影响比较小，那我们可以给一个比较小的权值，距离比较小的我们就可以给一个较大的权值</p>
<p>sklearn 中对于是否考虑距离，使用变量 weight，值为 unifrom（不考虑）和 distance（考虑）</p>
<p>我们同样使用 for 循环进行简单的搜索</p>
<p><img src="p.png" alt="p"></p>
<h4 id="明可夫斯基距离相应的-n"><a href="#明可夫斯基距离相应的-n" class="headerlink" title="明可夫斯基距离相应的 n"></a>明可夫斯基距离相应的 n</h4><p>sklearn 中对于明可夫斯基距离相应的 n 使用变量 p 表示，同样我们使用一个 for 循环搜索</p>
<p><img src="p_2.png" alt="p_2"></p>
<h4 id="其他的超参数"><a href="#其他的超参数" class="headerlink" title="其他的超参数"></a>其他的超参数</h4><p>可以查看 sklearn 官网对于 KNN 的算法的手册<br><a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html">http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</a></p>
